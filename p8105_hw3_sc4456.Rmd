---
title: "p8105_hw3_sc4456"
author: "Siyan Chen"
date: "10/5/2018"
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidyverse)
devtools::install_github("p8105/p8105.datasets")
library(p8105.datasets)

```

# Problem 1
data import
```{r}
data(brfss_smart2010) 
modified_brfss = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  mutate(response =factor(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"))) %>% arrange(response)


```

###1.1
```{r}
modified_brfss %>% 
  filter(year == "2002") %>% 
  group_by(locationabbr, locationdesc) %>% 
  summarise(number = n()) %>%
  # find the number of distinct locations within a state
  group_by(locationabbr) %>% 
  summarise(number= n()) %>% 
  # find the number of the same location occruing times
  filter(number == "7") 
```

In 2002, state CT, FL, NC were observed at 7 locations.

###1.2
```{r}
modified_brfss %>% 
  group_by(year, locationabbr) %>% 
  summarise(number = n()) %>% 
  ggplot(aes(x = year, y = number, color = locationabbr)) + geom_line()+
  theme_bw() + theme(legend.position = "right")
```

Based on the plot, the number of locations for each states varies over years and there is one state presents the dramatic changes. 

###1.3
```{r}
modified_brfss %>% 
  filter(locationabbr == "NY") %>% 
  filter(year == "2002" | year == "2006" | year == "2010") %>%
  select(year, locationdesc, response,data_value) %>% 
  filter(response == "Excellent") %>% 
  group_by(year) %>% 
  summarise(mean = mean(data_value), sd = sd(data_value)) %>% 
  knitr::kable()
```

Based on the table, year 2002 has greater mean proportion of excellent response compared to 2006 and 2010, but also has higher standard deviation. 

###1.4
```{r}
tidy_response_data =
modified_brfss %>% 
select(year, locationabbr, locationdesc, response, data_value) %>% 
group_by(locationabbr, year, response) %>% 
  summarise(average_proportion = mean(data_value, na.rm = TRUE)) 
  head(tidy_response_data)
  ggplot(tidy_response_data, aes(x = year, y = average_proportion, color = locationabbr)) + geom_point() + facet_grid(~response) + labs(y = "Average Proportion %")
```

According to the plot, the proportion of `Very good` response is higher compared to other response and the proportion of `Poor` response is smallest for all states. 


# Problem 2
```{r}
library(p8105.datasets)
instacart = data.frame(instacart) %>%
  janitor::clean_names() 
instacart%>% 
  distinct(aisle_id) %>% 
  nrow()
# getting the number of distinct aisles.
 instacart %>% 
  select(aisle) %>% 
  group_by(aisle) %>% 
  summarise(number = n()) %>% 
  # the number of orders from each aisles 
  mutate(ranking = min_rank(desc(number))) %>% 
  # ranking the the data with total order number from the highest to lowest
  filter(ranking == 1)
  # obtaining the aisle which are the most items ordered from.
```

Description: This dataset contains `r ncol(instacart)` variable and `r nrow(instacart)` observation. The structure is data frame. The key variables include product_id, product_name, aisle and others. For exmaple,user_id 112108 ordered Bulgarian Yogurt at Yogurt aisle which belongs to diary egg department.

###2.1 
There are 134 aisles and the fresh vegetables aisle is the most items ordered from fresh vegetables.

###2.2 plot

```{r}
library(ggplot2)
ordered_instacart = instacart %>% 
  group_by(aisle) %>% 
  summarise(number = n()) %>% 
  as.data.frame() %>% 
  mutate(aisle = reorder(aisle, desc(number))) 
  ggplot(ordered_instacart, aes(x = aisle, y = number)) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

Based on the plot, fresh vegetables and packaged vegetables are the most popular items which are ordered more than 100000 times. 

###2.3
```{r}
instacart %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle =="packaged vegetables fruits" ) %>% 
  select(aisle, product_name) %>% 
  group_by(aisle, product_name) %>% 
  summarise(number = n()) %>% 
  mutate(ranking = min_rank(desc(number))) %>% 
  # ranking from the most selled items number to the least within each aisle
  filter(ranking == 1) %>% 
# get the most popular prodcut for the interested aisles
  knitr::kable()
```

Based on the table, the most popular item of baking ingredients aisle is Light Brown Sugar which was ordered 499 times. The most popular item of dog food care aisle is Snack Sticks Chicken & Rice Recipe Dog Treats which was ordered 30 times.The most popular item of packaged vegetables fruits aisle is Organic Baby Spinach which was ordered 9784 times.

###2.4 

```{r}
instacart %>% 
  filter(product_name == "Pink Lady Apples"|product_name == "Coffee Ice Cream") %>% 
  select(product_name, order_hour_of_day, order_dow) %>% 
  # manipulate data
  group_by(product_name, order_dow) %>% 
  summarise(mean_hour = mean(order_hour_of_day)) %>% 
  mutate(order_dow = factor(order_dow, levels = 0:6, labels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))) %>% 
  spread(key = order_dow, value = mean_hour) %>% 
  knitr::kable()
```

The mean time of odering Coffee Ice Cream and Pink Lady Apples is similar on Sunday and Friday.

# Problem 3
```{r}
modified_ny_noaa = ny_noaa%>% 
  janitor::clean_names() %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  janitor::clean_names() %>% 
  as.data.frame() %>% 
 mutate(year = as.numeric(year), 
         month = as.numeric(month),
         day = as.numeric(day),
         prcp = as.numeric(prcp),
         snow = as.numeric(snow),
         snwd = as.numeric(snwd),
         tmax = as.numeric(tmax)/10, 
         tmin = as.numeric(tmin)/10)
head(modified_ny_noaa)
# Convert the meaningful cell to numerical value. The unit of original data contains tenths of degrees Cº and I convert it to degrees Cº by dividing 10.
```

This data contains `r ncol(modified_ny_noaa)` variables and `r nrow(modified_ny_noaa)` obervations. The key variables include `id` , `snow`, `tmin`, `tmax` and others. It is data frame structure. There are overall `r length(is.na(modified_ny_noaa))` NA. 

###3.1

```{r}
modified_ny_noaa%>% 
  group_by(snow) %>% 
  summarise(number = n()) %>% 
  arrange(desc(number))
# arrange the number of occuring snowfull value from the greatest to lowest and 0 is the most commonly occured
```

The Most commonly observed snowfall value is 0, because 0 is the most commonly occured value for varible snow.It is not often snow.


###3.2

```{r}
modified_ny_noaa%>% 
  filter(month == 1| month == 7) %>% 
  filter(!is.na(tmax)) %>% 
  group_by(id, month, year) %>% 
  summarise(average_max = mean(tmax)) %>% 
  ggplot(aes(x = year, y = average_max)) + geom_boxplot(aes(group = year)) + facet_grid(~month) 
```

According the the plot, July month generally has much higher temperature compared to January which makes sense. There are outliers which presents the abnormally high temperature at January and abnormally low temperature at July. There are also outliers within stations for each year which suggest the variability of temperature.

###3.3
```{r}
library(patchwork)
tem_plot = modified_ny_noaa%>% 
  filter(!is.na(tmax) , !is.na(tmin)) %>% 
  ggplot(aes(x = tmax, y = tmin)) + geom_hex() 
snow_plot = modified_ny_noaa%>% 
   filter(snow > 0 & snow < 100) %>% 
  ggplot(aes(x = year, y = snow)) + geom_boxplot(aes(group = year)) 
tem_plot + snow_plot
```

Based on the first plot, the combination of tmax and tmin occure at around 0 and 30 degree most often. Based on the second plot, the distribution of snow which greater than 0 and smaller than 100 is similar across years. There are some certain year present the unusuly pattern and outliers.












